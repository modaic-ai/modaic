---
title: Language Models
---
You must configure an LM for your program to run.

## Creating an LM
Creating an LM is easy. Just pass in the model name into the `dspy.LM` constructor. 
The model names are the same as the ones used by litellm so if you are ever confused about how to use a specific model provider refer to their [docs](https://docs.litellm.ai/docs/providers).
```python
lm = dspy.LM(model="openai/gpt-4o")
```

## Using an LM
### Global
The most common way to use an LM is by passing it into dspy settings so all programs will use it by default.
```python
dspy.configure(lm=dspy.LM(model="openai/gpt-4o"))
predictor = dspy.Predict("question -> answer")
result = predictor(question="What is the capital of France?")
```
Or for a single thread/coroutine
```python
predictor = dspy.Predict("question -> answer")
with dspy.context(lm=dspy.LM(model="openai/gpt-4o")):
    result = predictor(question="What is the capital of France?")
```

### For a single program
If you want only a specific program to use an LM you can use .set_lm() to make its predictors use that LM.

```python
predictor = dspy.Predict("question -> answer")
predictor.set_lm(lm=dspy.LM(model="openai/gpt-4o"))
```

### Standalone 
This is a rare use case but you can always use the LM directly without any modules to call it.
```python
lm = dspy.LM(model="openai/gpt-4o")
result = lm(messages=[{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the capital of France?"}])
# Or with just single prompt
# result = lm("What is the capital of France?")
print(result)
>>> {"role": "assistant", "content": "The capital of France is Paris."}
```

**Asyncronous Call**
```python
result = await lm.acall(messages=[{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the capital of France?"}])
# Or with just single prompt
# result = await lm.acall("What is the capital of France?")
print(result)
>>> {"role": "assistant", "content": "The capital of France is Paris."}
```